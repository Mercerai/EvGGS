# EvGGS
Coda and Dataset of  EvGGS: A Collaborative Learning Framework for Event-based Generalizable Gaussian Splatting.
Jiaxu Wang, Junhao He, Ziyi Zhang, Mingyuan Sun, Jingkai Sun, Renjing Xu

# Dataset

# Ev3D-S
a large-scale event dataset with varying textures and materials and accompanied by well-calibrated frame, depth, and silhouette groundtruth.

Download the dataset from OneDrive...


# EV3D-R
A large Event-based 3D dataset captured by a real event camera of EvGGS

Download the datasets from [Baidu Disk](https://pan.baidu.com/s/1EuR-l_b_g-j_Du6dOxtZEg?pwd=3ilt ), code:3ilt

Download the datasets from [OneDrive](https://hkustgz-my.sharepoint.com/:u:/g/personal/junhaohe_hkust-gz_edu_cn/EY__SmcUSbdFs13sb2h8svYBXYOCDd0OVnSWV-WLfvFLmA?e=GkVjhd)

You can construct your custom datasets by the following steps:

1. Render the scenario in Blender:
    - Put the object in the origin and let the camera trace bounded by a unit sphere.
    - The groundtruth pose, depth, and optical flow are generated by VisionBlender. 
2. Record the event stream using your event cameras or convert the RGB image series using the [event converter](https://github.com/SensorsINI/v2e)
3. Align the groundtruth poses, depth, optical flows, and grayscale images with event streams.
4. Dataset formats:
    - H5_Files: The aligned event streams stored in H5 files
    - Render_info: Grayscale images and .npz files which contain poses, depth, and optical flows.
    - Intrinsics: Camera intrinsics

Please contact us if you need to construct your custom event datasets.
